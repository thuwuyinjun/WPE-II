\section{Introduction}

%problem description
Machine learning algorithms have been broadly applied in various data analysis applications in recent years, which pose new challenges in various aspects. One of such challenge is that the collections of data for training or prediction are not only huge but also constantly updated while the real-time performance is expected in many applications. Typical examples include online clustering analysis for clickstream \cite{guha2000clustering}, incremental collaborative filtering analysis for recommendation systems \cite{papagelis2005incremental} etc. Furthermore, the analytic workloads for building machine learning models or making predictions over the data may also arrive in a continuous manner. Considering the possible similarity between the requests among the workloads, whether it is possible to cache and reuse those computation results so as to reduce the computation overhead is another concern for users.
% All of those applications and solutions share the same spirit, i.e. {\em computing new machine learning model incrementally without retraining the model from the scratch.}

%Database solutions for it
Similar situation happens in database community, where database instances are large and versioned and thus {\em database view} is materialized for efficient query response \cite{date2006relational}, maintained to reflect changes from the underlying base relations \cite{gupta1993maintaining, green2007update} and reused to reduce query execution overhead \cite{halevy2001answering}, which plays an important role in data integration \cite{levy1996querying}, query optimization \cite{rajaraman1995answering}, data visualization \cite{brachman1993integrated}. 

Since such view-related problem has been extensively studied from the both theoretical and systematical side in database domain, \eat{which targets at improving the query performance,} it is natural to apply those ideas to data analysis tasks by regarding some pre-computed results as view. Then two major research problems on this kind of special views have emerged in recent applications, i.e. 1) how to {\em incrementally update} the those pre-computed results in an efficient way in the case of update to the data over which those results are constructed; 2) how to {\em reuse} those pre-computed results for the following computations, which are closely related to traditional {\em incremental view maintenance problem} and {\em query rewriting using views problem} from database community respectively. which have been explored by recent works in database communities. 

Both of the two research problems can span in different lines, depending on which kinds of pre-computed results are materialized as ``views''. Considering the fact that there are two typical computation phases in typical machine learning pipelines, i.e. training phase and test phase, the views can be the result of training phase, i.e. the {\em model parameters}, or the output of test phase, i.e. the {\em classification results}. Besides, since the two computation phases are composed of a series of linear algebra operations, the idea of materializing their results can also be extended to more {\em general linear algebra programs}. 

In terms of the first research problem, materialization of the pre-computed results can appear in all of the three scenarios mentioned above. Specifically, \cite{deshpande2006mauvedb} and \cite{gupta2015processing} are on incrementally updating the materialized model parameters, \cite{koc2011incrementally} is on incrementally updating the materialized classification results while \cite{nikolic2014linview} is on incrementally updating the output of the linear algebra programs, which rely on some assumptions in different aspects. One typical assumption for those works is about the updates against the pre-computed results, in which the type of updates can be addition, removal or modifications to the input data, the amount of updates can be small or large or arbitrary and the updates can even appear in different dimensions, i.e. feature level and sample level. Table \ref{tab:assump_updates} gives an overview of all of the papers to be discussed along with the assumptions about updates that they follow. For example, for \cite{nikolic2014linview}, the updates should be small modifications to the input data rather than addition or removal of input data, which, however, can appear at both sample level and feature level. There are also some other minor assumptions for specific cases in those works. For example, in \cite{koc2011incrementally}, the goal is to incrementally update the prediction results in the test phase. So the time to update the model parameters is assumed to be negligible. Another example is in \cite{nikolic2014linview}, in which the linear algebra programs are assumed to be composed of pure linear operations without any other complex non-linear operations. More discussions will be expanded in the following sections.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}\hline
        % && \multicolumn{3}{|c|}{Eager {\em Update} (updates/s)}&\multicolumn{3}{|c|}{Lazy {\em All members} (scan/s)}  \\\hhline{~~------}
        % &&Re-evaluation & Incremental\\ \hline
        &&Sample level& Feature level\\\hline
    \multirow{2}{*}{amount of updates}&small&\cite{deshpande2006mauvedb, gupta2015processing,koc2011incrementally, nikolic2014linview}  & \cite{nikolic2014linview} \\\hhline{~---}
    &arbitrary&\makecell{linear regression in \cite{deshpande2006mauvedb, gupta2015processing}\\ naive bayes in \cite{gupta2015processing}} & \\\hline
    % \multicolumn{2}{|c|}{Hybrid}&2.0 & 6.6 & 0.2 & 8.0 & 48.8 & 2.1\\\hline
    \multirow{3}{*}{type of updates}&addition of data&\cite{deshpande2006mauvedb, gupta2015processing,koc2011incrementally} &  \\\hhline{~---}
    &removal of data&\makecell{linear regression in \cite{deshpande2006mauvedb, gupta2015processing}\\ naive bayes in \cite{gupta2015processing}} &  \\\hhline{~---}
    &modifications to data&\cite{nikolic2014linview} & \cite{nikolic2014linview} \\\hline
    % \multirow{3}{*}{Space}&Linear & $n^2$ & $n^2k$\\\hhline{~---}
    % &Exponential & $n^2$ & $n^2log k$\\\hhline{~---}
    % &Skip-s & $n^2$ & $n^2(logs+\frac{k}{s})$\\\hline
    \end{tabular}
    \caption{Assumptions about updates}
    \label{tab:assump_updates}
\end{table}

For the second research problem, the pre-computed results to be dealt with are only the machine learning models, which is primarily explored by \cite{gupta2015processing} and \cite{hasani2018efficient}. In those two papers, the pre-computed machine learning models are reused to construct approximated models for user's requests in two different manners, i.e., merging pre-computed model parameters directly and combining the coresets \cite{agarwal2005geometric} of the pre-computed model parameters, for the purpose of saving the computation time. In order to avoid significant differences between the constructed models and the models built from the scratch, there should exist some approximation rate for combined models, which depends on the properties of specific type of machine learning models, such as the approximation properties on coresets \cite{agarwal2005geometric}. It indicates that only for the models with such approximation properties such as K-means, Gaussian mixture models, SVM and logistic regression, the solutions from \cite{gupta2015processing} are applicable. 

Besides, the second research problem is also coherent to incremental model parameters update problem as mentioned before. For those types of machine learning models over which the updates can be removal of data (see Table \ref{tab:assump_updates}), when combining the pre-computed machine learning models, their effect can be either accumulated or canceled while for other types of models introduced in \cite{deshpande2006mauvedb, gupta2015processing,koc2011incrementally}, the combinations of models only means the ``addition'' of the effect of those model. More details are presented in the following sections.

This article is organized as follow, in Section \ref{sec: pre}, some basic notations and concepts for various machine learning algorithms are introduced, which prepares for the following discussions for incremental maintenance for pre-computed results in Section \ref{sec: maintain_views} and reuse of pre-computed results in Section \ref{sec: view_reuse} in the context of data analytics.